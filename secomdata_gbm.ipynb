{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Gradient Boosting Classifier for an imbalanced data set\n",
    "\n",
    "Date created: Jan 11, 2017   \n",
    "Last modified: Jan 12, 2017  \n",
    "Tags: GBM, hyperopt, imbalanced data set, semiconductor data   \n",
    "About: Classify imbalanced semicondutor manufacturing data set using the Gradient Boosting Machine. Weight imbalanced classes using `sample_weight`. Tune hyperparameters manually and algorithmically using `hyperopt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>I. Introduction</h3>\n",
    "\n",
    "The [SECOM dataset](http://archive.ics.uci.edu/ml/datasets/SECOM) in the  [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml) is semicondutor manufacturing data. There are 1567 records, 590 anonymized features and 104 fails. This makes it an imbalanced dataset with a 14:1 ratio of pass to fails. The process yield has a simple pass/fail response (encoded -1/1).\n",
    "\n",
    "<h4>Objective</h4>\n",
    "We consider some of the different approaches to classify imbalanced data. There are two strategies available when working with highly imbalanced classes:  \n",
    "\n",
    "- cost sensitive learning (penalizing misclassification of the minority class)\n",
    "- resampling to balance the classes\n",
    "\n",
    "In previous exercises we looked at the [one-class SVM](https://github.com/Meena-Mani/SECOM_class_imbalance/blob/master/secomdata_ocsvm.ipynb) and [SVM+oversampling using SMOTE](https://github.com/Meena-Mani/SECOM_class_imbalance/blob/master/secomdata_svm_smote.ipynb). (The full list of methods we have looked can be found [here](https://github.com/Meena-Mani/SECOM_class_imbalance/blob/master/README.md)). In this exercise we consider the [Gradient Boosting (GBM)](https://en.wikipedia.org/wiki/Gradient_boosting) classifier which is an ensemble method that has been shown to [perform very well](https://www.youtube.com/watch?v=GTs5ZQ6XwUM#t=7m04s) in machine learning competitions. To implement a cost-sensitive GBM, the GBM `fit` method with the `sample_weight` option is used to weigh individual observations.  \n",
    " \n",
    "<h4>Methodology</h4>\n",
    "The *sklearn* [GBM Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) module uses the `sample_weight` mask to assign a vector of weights to corresponding observations. To balance the two classes, we will weigh them in inverse proportion to their respective class frequencies. \n",
    "\n",
    "At each stage, the boosting algorithm will fit the residuals from the previous step. We need to determine if the data needs to be reweighted in order to get optimal results. We will look at:\n",
    "- the baseline case (with equal class weights)\n",
    "- sample weights (in inverse proportion to the class frequencies)  \n",
    "\n",
    "In addition, since hyperparameter tuning is an important part of GBM performance, we will look at:\n",
    "- Manual selection of parameters\n",
    "- Hyperparameter optimization with cross validation using `hyperopt`\n",
    "- Grid search optimization with cross validation  \n",
    "\n",
    "We will see if any of these three parameter selection methods suggest the use of default versus sample weights.\n",
    "\n",
    "<h4>Preprocessing</h4>\n",
    "Like the Random Forest (RF) classifier, the GBM can handle continuous and categorical varibles and does not require transformations or preprocessing such as one-hot encoding or scaling. The only preprocessing needed is with regards to the missing data.\n",
    "For this data set, the measurements come from a large number of processes or sensors and many of the records are missing. In addition some measurements are identical/constant and so not useful for prediction. We will remove the columns that have high missing counts or constant values and estimate values for the rest of the missing data. These are the same steps used for the [one-class SVM](https://github.com/Meena-Mani/SECOM_class_imbalance/blob/master/secomdata_ocsvm.ipynb) and a more detailed explanation can be seen there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    " \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report,\\\n",
    "roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef\n",
    "\n",
    "from time import time\n",
    "from __future__ import division\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 7  # random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 1567 observations/rows and 590 variables/columns.\n",
      "The ratio of majority class to minority class is 14:1.\n"
     ]
    }
   ],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\"\n",
    "secom = pd.read_table(url, header=None, delim_whitespace=True)\n",
    "\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data\"\n",
    "y = pd.read_table(url, header=None, usecols=[0], squeeze=True, delim_whitespace=True)\n",
    "\n",
    "print 'The dataset has {} observations/rows and {} variables/columns.' \\\n",
    "       .format(secom.shape[0], secom.shape[1])\n",
    "print 'The ratio of majority class to minority class is {}:1.' \\\n",
    "      .format(int(y[y == -1].size/y[y == 1].size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>II. Preprocessing </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the missing values first, dropping columns which have a large number of missing values and imputing values for those that have only a few missing values.\n",
    "The [one-class SVM](https://github.com/Meena-Mani/SECOM_class_imbalance/blob/master/secomdata_ocsvm.ipynb) exercise has a more detailed version of these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SECOM data set now has 409 variables.\n"
     ]
    }
   ],
   "source": [
    "# dropping columns which have large number of missing entries \n",
    "\n",
    "m = map(lambda x: sum(secom[x].isnull()), xrange(secom.shape[1]))\n",
    "m_200thresh = filter(lambda i: (m[i] > 200), xrange(secom.shape[1]))\n",
    "secom_drop_200thresh = secom.dropna(subset=[m_200thresh], axis=1)\n",
    "dropthese = [x for x in secom_drop_200thresh.columns.values if \\\n",
    "             secom_drop_200thresh[x].std() == 0]\n",
    "secom_drop_200thresh.drop(dropthese, axis=1, inplace=True)\n",
    "\n",
    "print 'The SECOM data set now has {} variables.'\\\n",
    "      .format(secom_drop_200thresh.shape[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imputing missing values for the random forest\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "secom_imp = imp.fit_transform(secom_drop_200thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>III. GBM: baseline vs using sample_weight</h3>  \n",
    "\n",
    "We will first compare baseline results with the performance of a model where the `sample_weight` is used. As discussed in [previous exercises](https://github.com/Meena-Mani/SECOM_class_imbalance/blob/master/README.md), the <i>Matthews correlation coefficient (MCC)</i> is used instead of the <i>Accuracy</i> to compute the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into train and holdout sets\n",
    "# stratify the sample used for modeling to preserve the class proportions\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(secom_imp, y, \\\n",
    "                                       test_size=0.2, stratify=y, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to test GBC parameters\n",
    "\n",
    "def GBC(params, weight):\n",
    "    \n",
    "    clf = GradientBoostingClassifier(**params)\n",
    "                            \n",
    "    if weight:\n",
    "        sample_weight = np.array([14 if i == -1 else 1 for i in y_train])\n",
    "        clf.fit(X_train, y_train, sample_weight)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    print_results(clf, X_train, X_test)\n",
    "\n",
    "    \n",
    "# function to print results\n",
    "def print_results(clf, X_train, X_test):        \n",
    "    # training data results  \n",
    "    print 'Training set results:'\n",
    "    y_pred = clf.predict(X_train)\n",
    "    print 'The train set MCC: {0:4.3f}'\\\n",
    "    .format(matthews_corrcoef(y_train, y_pred))\n",
    "    \n",
    "    # test set results\n",
    "    print '\\nTest set results:'\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    print(\"Accuracy: {:.4f}\".format(acc))\n",
    "                             \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print '\\nThe confusion matrix: '\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print cm\n",
    "    print '\\nThe test set MCC: {0:4.3f}'\\\n",
    "    .format(matthews_corrcoef(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>A) Baseline</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set results:\n",
      "The train set MCC: 0.987\n",
      "\n",
      "Test set results:\n",
      "Accuracy: 0.9331\n",
      "\n",
      "The confusion matrix: \n",
      "[[291   2]\n",
      " [ 19   2]]\n",
      "\n",
      "The test set MCC: 0.197\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 800, 'max_depth': 3, 'subsample': 0.8, 'max_features' : 'sqrt',\n",
    "          'learning_rate': 0.019, 'min_samples_split': 2, 'random_state': SEED}\n",
    "GBC(params, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>B) Sample weight</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set results:\n",
      "The train set MCC: 0.608\n",
      "\n",
      "Test set results:\n",
      "Accuracy: 0.9140\n",
      "\n",
      "The confusion matrix: \n",
      "[[285   8]\n",
      " [ 19   2]]\n",
      "\n",
      "The test set MCC: 0.097\n"
     ]
    }
   ],
   "source": [
    "# RUN 1 Using the same parameters as the baseline\n",
    "\n",
    "params = {'n_estimators': 800, 'max_depth': 3, 'subsample': 0.8, 'max_features' : 'sqrt', \n",
    "          'learning_rate': 0.019, 'min_samples_split': 2, 'random_state': SEED}\n",
    "GBC(params, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set results:\n",
      "The train set MCC: 0.664\n",
      "\n",
      "Test set results:\n",
      "Accuracy: 0.9331\n",
      "\n",
      "The confusion matrix: \n",
      "[[290   3]\n",
      " [ 18   3]]\n",
      "\n",
      "The test set MCC: 0.242\n"
     ]
    }
   ],
   "source": [
    "# RUN 2 Manually selecting parameters to optimize the train/test MCC with sample weights\n",
    "\n",
    "params = {'n_estimators': 800, 'max_depth': 3, 'subsample': 0.7, 'max_features' : 'log2', \n",
    "          'learning_rate': 0.018, 'min_samples_split': 2, 'random_state': SEED}\n",
    "\n",
    "GBC(params, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the baseline case (where we do not adjust the weights), we get a high MCC score for the training set (0.97). The test MCC is 0.197 so there is a large gap between the train and test MCC. When sample weights are used, we get a test set MCC of 0.242 after tuning the parameters. \n",
    "\n",
    "The tuning parameters play a big role in the performance of the GBM. In Section III (D) we will look at the MCC trend over the number of estimators. This will allow us to adjust the complexity of the model. In Sections IV and V  we will use the `hyperopt` and `gridsearchCV` modules to select the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>C) Feature importance</h4>  \n",
    "\n",
    "Here we compute the feature importance (using the baseline parameters) for the GBM. We also compare the results with the feature importance for the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           GBM                   RF           \n",
      "     Feature # Importance Feature # Importance\n",
      "Rank                                          \n",
      "1           61   0.022117        55   0.016726\n",
      "2           56   0.018574        50   0.010880\n",
      "3           55   0.017534        56   0.010454\n",
      "4          131   0.015305       265   0.009200\n",
      "5           50   0.013360       131   0.008128\n",
      "6           22   0.012942       130   0.008107\n",
      "7          311   0.012862       203   0.008095\n",
      "8           43   0.011589       311   0.007529\n",
      "9          203   0.010597       106   0.007180\n",
      "10          17   0.010526       321   0.007113\n",
      "11          65   0.009741       230   0.007104\n",
      "12         177   0.009733       269   0.006803\n",
      "13         267   0.009610        89   0.006785\n",
      "14          85   0.009479       176   0.006750\n",
      "15         230   0.009351       250   0.006571\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 800, 'max_depth': 3, 'subsample': 0.8, 'max_features' : 'sqrt', \n",
    "          'learning_rate': 0.019, 'min_samples_split': 2, 'random_state': SEED}\n",
    "\n",
    "# GBM\n",
    "clf = GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "gbm_importance = clf.feature_importances_\n",
    "gbm_ranked_indices = np.argsort(clf.feature_importances_)[::-1]\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=7)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_importance = rf.feature_importances_\n",
    "rf_ranked_indices = np.argsort(rf.feature_importances_)[::-1]\n",
    "\n",
    "# printing results in a table\n",
    "importance_results = pd.DataFrame(index=range(1,16), \n",
    "                                  columns=pd.MultiIndex.from_product([['GBM','RF'],['Feature #','Importance']]))\n",
    "importance_results.index.name = 'Rank'\n",
    "importance_results.loc[:,'GBM'] =  list(zip(gbm_ranked_indices[:15], \n",
    "                                            gbm_importance[gbm_ranked_indices[:15]]))\n",
    "importance_results.loc[:,'RF'] =  list(zip(rf_ranked_indices[:15], \n",
    "                                           rf_importance[rf_ranked_indices[:15]]))\n",
    "print importance_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly half the top fifteen most important features for the GBM were also the top fifteen computed for the Random Forest classifier. There are complex interactions between the parameters so we do not expect the two classifiers to give the same results. In Section IV where we optimize the hyperparameters, the `nvar` (number of variables) parameter will select variables according to their ranked importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>D) Number of estimators -- model complexity</h4>\n",
    "\n",
    "The GBM sequentially fits the function and the number of steps (the number of estimators) is specified by the `ntree` parameter. At some stage, the model complexity increases to the point where we start overfitting the data. Here we will plot the MCC as a function of the number of estimators for the train set and test set to determine a good value for `ntree`.   \n",
    "\n",
    "When <i>Accuracy</i> is used as score, a plot of <i>Classification Error</i> (= <i>1 - Accuracy</i>) versus the <i>Number of Trees</i> can be used as a diagnostic to determine bias and overfitting. The term <i>(1 - MCC)</i>, however, is hard to interpret since MCC is calculated with all four terms of the confusion matrix. (I did make those plots out of curiosity and the trends are similar to those seen in an \"Error vs no. of estimators\" plot an example of which is presented on Slide 3 of [this](https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/cv_boot.pdf) Hastie/Tibshirani lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to compute MCC vs number of trees\n",
    "\n",
    "def GBC_trend(weight):\n",
    "    base_params = {'max_depth': 3, 'subsample': 0.8, 'max_features' : 'sqrt',\n",
    "                   'learning_rate': 0.019, 'min_samples_split': 2, 'random_state': SEED}\n",
    "\n",
    "    mcc_train = []\n",
    "    mcc_test = []\n",
    "\n",
    "    for i in range(500, 1600, 100):\n",
    "        params = dict(base_params)\n",
    "        ntrees = {'n_estimators': i}\n",
    "        params.update(ntrees)\n",
    "    \n",
    "        clf = GradientBoostingClassifier(**params)\n",
    "        if weight:\n",
    "            sample_weight = np.array([14 if i == -1 else 1 for i in y_train])\n",
    "            clf.fit(X_train, y_train, sample_weight)\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "    \n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        mcc_train.append(matthews_corrcoef(y_train, y_pred_train))\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        mcc_test.append(matthews_corrcoef(y_test, y_pred_test))\n",
    "    \n",
    "    return mcc_train, mcc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAEaCAYAAABTpC6GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1055cce90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(7,4))\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "mcc_train, mcc_test = GBC_trend(0)\n",
    "ax[0].plot(range(500, 1600, 100), mcc_train, color='magenta', label='train MCC' )\n",
    "ax[0].plot(range(500, 1600, 100), mcc_test, color='olive', label='test MCC' )\n",
    "ax[0].set_title('For default weights')\n",
    "\n",
    "mcc_train, mcc_test = GBC_trend(1)\n",
    "ax[1].plot(range(500, 1600, 100), mcc_train, color='magenta', label='train MCC' )\n",
    "ax[1].plot(range(500, 1600, 100), mcc_test, color='olive', label='test MCC' )\n",
    "ax[1].set_title('For sample weights')\n",
    "\n",
    "ax[0].set_ylabel('MCC')\n",
    "fig.text(0.5, 0.04, 'Boosting Iterations', ha='center', va='center')\n",
    "plt.xlim(500,1500)\n",
    "plt.ylim(-0.1, 1.1);\n",
    "plt.legend(bbox_to_anchor=(1.05, 0), loc='lower left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default weight option (left): After about 900 iterations, the GBM models all the training data perfectly. At the same time, there is a large gap in the holdout data classification results.  This is a classic case of overfitting.  \n",
    "\n",
    "Sample weight option (right): This plot was constructed using the same parameters at the default weight plot. This may account for some of the bias (lower values for MCC) we see for the training data line. From III B, we can see that even when we tune the parameters for the sample weight model, the MCC for the training set is lower that for the default weight model. This gives us some indication that using sample weights adds bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>IV. Hyperparameter optimization using Hyperopt</h3>  \n",
    "\n",
    "The python `hyperopt` module by James Bergstra uses Bayesian optimization (based on a tree-structured Parzen density estimate--TPE) <a href=\"#ref1\">[1]</a> to automatically select the best hyperparameters. [This](https://districtdatalabs.silvrback.com/parameter-tuning-with-hyperopt) blog post provides examples of how this can be used with <i>sklearn</i> classifiers. We will use the <i>MCC</i> instead of the <i>Accuracy</i> for the cross-validation score. Note that since the `hyperopt` function `fmin` will minimze the MCC, we need to negate the MCC to maximize its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining the MCC metric to assess cross-validation\n",
    "\n",
    "def mcc_score(y_true, y_pred):\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    return mcc\n",
    "    \n",
    "mcc_scorer = make_scorer(mcc_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to DataFrame for easy indexing of number of variables (nvar)\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "def hyperopt_train_test(params):\n",
    "    \n",
    "    weight = params['weight']\n",
    "    X_ = X_train.loc[:, gbm_ranked_indices[:params['nvar']]]\n",
    "    del params['nvar'], params['weight']\n",
    "   \n",
    "    clf = GradientBoostingClassifier(**params)\n",
    "    if weight:\n",
    "        sample_weight = np.array([14 if i == -1 else 1 for i in y_train])\n",
    "        return cross_val_score(clf, X_, y_train, scoring=mcc_scorer,\\\n",
    "                               fit_params={'sample_weight': sample_weight}).mean()\n",
    "    else: \n",
    "       return cross_val_score(clf, X_, y_train, scoring=mcc_scorer).mean()\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', range(700,1300, 100)),\n",
    "    'max_features': hp.choice('max_features',  ['sqrt', 'log2']),\n",
    "    'max_depth': hp.choice('max_depth', range(2,5)),\n",
    "    'subsample': hp.choice('subsample', [0.6, 0.7, 0.8, 0.9]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 3]), \n",
    "    'learning_rate': hp.choice('learning_rate', [0.01, 0.015, 0.018, 0.02, 0.025]),\n",
    "    'nvar': hp.choice('nvar', [200, 300, 400]),\n",
    "    'weight': hp.choice('weight', [0, 1]),  # select sample_weight or default\n",
    "    'random_state': SEED\n",
    " }  \n",
    "\n",
    "def f(params):\n",
    "    mcc = hyperopt_train_test(params)\n",
    "    # with a negative sign, we maximize the MCC\n",
    "    return {'loss': -mcc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Run 1</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperoptCV took 369.96 seconds.\n",
      "\n",
      "Best parameters (by index):\n",
      "{'weight': 1, 'nvar': 0, 'learning_rate': 2, 'n_estimators': 5, 'subsample': 1, 'min_samples_split': 1, 'max_features': 1, 'max_depth': 1}\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "trials = Trials()\n",
    "best = fmin(f, space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "print(\"HyperoptCV took %.2f seconds.\"% (time() - start))\n",
    "print '\\nBest parameters (by index):'\n",
    "print best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the optimal hyperparameters selected via `hyperopt` above to the GBM classifier. The optimal parameters include `nvar`= 200 and the use of `sample_weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set results:\n",
      "The train set MCC: 0.773\n",
      "\n",
      "Test set results:\n",
      "Accuracy: 0.8949\n",
      "\n",
      "The confusion matrix: \n",
      "[[280  13]\n",
      " [ 20   1]]\n",
      "\n",
      "The test set MCC: 0.004\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 1200, 'max_depth': 3, 'subsample': 0.7, 'max_features' : 'log2', \n",
    "          'learning_rate': 0.018, 'min_samples_split': 3, 'random_state': SEED}\n",
    "\n",
    "train_ = X_train.loc[:, gbm_ranked_indices[:200]]\n",
    "test_ = X_test.loc[:, gbm_ranked_indices[:200]]\n",
    "\n",
    "clf = GradientBoostingClassifier(**params)\n",
    "sample_weight = np.array([14 if i == -1 else 1 for i in y_train])\n",
    "clf.fit(train_, y_train, sample_weight)\n",
    "        \n",
    "print_results(clf, train_, test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Run 2</h4>  \n",
    "I repeated the run with `hyperopt` a few times and in each case the optimal parameters include `nvar`= 200 and the use of `sample_weight`.  There is a great deal of variability among the remaining parameters selected across the runs. This is an example of a second run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters (by index):\n",
      "{'weight': 1, 'nvar': 0, 'learning_rate': 2, 'n_estimators': 0, 'subsample': 2, 'min_samples_split': 0, 'max_features': 1, 'max_depth': 2}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(f, space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "print '\\nBest parameters (by index):'\n",
    "print best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set results:\n",
      "The train set MCC: 0.741\n",
      "\n",
      "Test set results:\n",
      "Accuracy: 0.9045\n",
      "\n",
      "The confusion matrix: \n",
      "[[282  11]\n",
      " [ 19   2]]\n",
      "\n",
      "The test set MCC: 0.072\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 700, 'max_depth': 4, 'subsample': 0.8, 'max_features' : 'log2', \n",
    "          'learning_rate': 0.018, 'min_samples_split': 2, 'random_state': SEED}\n",
    "\n",
    "train_ = X_train.loc[:, gbm_ranked_indices[:200]]\n",
    "test_ = X_test.loc[:, gbm_ranked_indices[:200]]\n",
    "\n",
    "clf = GradientBoostingClassifier(**params)\n",
    "sample_weight = np.array([14 if i == -1 else 1 for i in y_train])\n",
    "clf.fit(train_, y_train, sample_weight)\n",
    "        \n",
    "print_results(clf, train_, test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Run 3 -- default weight</h4>  \n",
    "For both Run 1 and Run 2, `hyperopt` selected the `sample_weight` option. This was the case for most of the runs though there were a few instances in which the default option was selected. This is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperoptCV took 370.07 seconds.\n",
      "\n",
      "Best parameters (by index):\n",
      "{'weight': 0, 'nvar': 0, 'learning_rate': 4, 'n_estimators': 3, 'subsample': 3, 'min_samples_split': 1, 'max_features': 1, 'max_depth': 0}\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "trials = Trials()\n",
    "best = fmin(f, space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "print(\"HyperoptCV took %.2f seconds.\"% (time() - start))\n",
    "print '\\nBest parameters (by index):'\n",
    "print best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set results:\n",
      "The train set MCC: 0.913\n",
      "\n",
      "Test set results:\n",
      "Accuracy: 0.9204\n",
      "\n",
      "The confusion matrix: \n",
      "[[287   6]\n",
      " [ 19   2]]\n",
      "\n",
      "The test set MCC: 0.119\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 1000, 'max_depth': 2, 'subsample': 0.9, 'max_features' : 'log2', \n",
    "          'learning_rate': 0.025, 'min_samples_split': 3, 'random_state': SEED}\n",
    "\n",
    "train_ = X_train.loc[:, gbm_ranked_indices[:200]]\n",
    "test_ = X_test.loc[:, gbm_ranked_indices[:200]]\n",
    "\n",
    "clf = GradientBoostingClassifier(**params)\n",
    "clf.fit(train_, y_train)\n",
    "    \n",
    "print_results(clf, train_, test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from `hyperopt` (tested over ten or more runs) were quite variable and no conclusions can be made. By seeding the `random_state` parameter, we should be able to get reproducible results but since this was not the case here, we will need to investigate this further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>V. Grid search with cross-validation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cv function\n",
    "\n",
    "def GBMCV(weight):\n",
    "    \n",
    "    clf = GradientBoostingClassifier(random_state=SEED)\n",
    "\n",
    "    param_grid = {\"n_estimators\": [800, 900, 1000, 1200],\n",
    "                  \"max_depth\": [2, 3],\n",
    "                  \"subsample\": [0.6, 0.7, 0.8],\n",
    "                  \"min_samples_split\": [2, 3],\n",
    "                  \"max_features\": [\"sqrt\", \"log2\"],\n",
    "                  \"learning_rate\": [0.01, 0.015, 0.018, 0.02, 0.025]}\n",
    "              \n",
    "    # run grid search\n",
    "    if weight:\n",
    "        sample_weight = np.array([14 if i == -1 else 1 for i in y_train])\n",
    "        grid_search = GridSearchCV(clf, param_grid=param_grid,  scoring=mcc_scorer, \\\n",
    "                                   fit_params={'sample_weight': sample_weight})\n",
    "    else:\n",
    "        grid_search = GridSearchCV(clf, param_grid=param_grid,  scoring=mcc_scorer)\n",
    "        \n",
    "    start = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # print results\n",
    "    print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "     % (time() - start, len(grid_search.grid_scores_)))\n",
    "\n",
    "    print 'The best parameters:'\n",
    "    print '{}\\n'. format(grid_search.best_params_)\n",
    "\n",
    "    print 'Results for model fitted with the best parameter:'\n",
    "    y_true, y_pred = y_test, grid_search.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    print 'The confusion matrix: '\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print cm\n",
    "\n",
    "    print '\\nThe Matthews correlation coefficient: {0:4.3f}'\\\n",
    "    .format(matthews_corrcoef(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with default weights:\n",
      "GridSearchCV took 1802.75 seconds for 480 candidate parameter settings.\n",
      "The best parameters:\n",
      "{'subsample': 0.7, 'learning_rate': 0.025, 'n_estimators': 1000, 'min_samples_split': 3, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "\n",
      "Results for model fitted with the best parameter:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.94      0.99      0.96       293\n",
      "          1       0.25      0.05      0.08        21\n",
      "\n",
      "avg / total       0.89      0.93      0.90       314\n",
      "\n",
      "The confusion matrix: \n",
      "[[290   3]\n",
      " [ 20   1]]\n",
      "\n",
      "The Matthews correlation coefficient: 0.083\n"
     ]
    }
   ],
   "source": [
    "# CV with default weights\n",
    "\n",
    "print \"CV with default weights:\"\n",
    "GBMCV(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters:\n",
      "{'subsample': 0.8, 'learning_rate': 0.02, 'n_estimators': 800, 'min_samples_split': 3, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "\n",
      "Results for model fitted with the best parameter:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.93      0.96      0.95       293\n",
      "          1       0.08      0.05      0.06        21\n",
      "\n",
      "avg / total       0.88      0.90      0.89       314\n",
      "\n",
      "The confusion matrix: \n",
      "[[282  11]\n",
      " [ 20   1]]\n",
      "\n",
      "The Matthews correlation coefficient: 0.013\n"
     ]
    }
   ],
   "source": [
    "# CV with sample weights\n",
    "\n",
    "GBMCV(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>VI. Summary</h3>\n",
    "\n",
    "In this exercise the GBM was used to classify imbalanced data. In looking at the GBM classifier we need to consider the following:\n",
    "(1) whether the GBM is superior to other methods we have looked at and\n",
    "(2) whether reweighting with sample weights will improve classification results\n",
    "\n",
    "On the issue of the GBM classifier performance (classification results/time/ease of use) it should be noted that tuning the GBM is no simple matter as there are many complex interactions between the parameters. We use the three methods -- manual selection, automatic hyperparameter optimization using a Bayesian strategy and grid search cross validation -- available but they give variable results. At the same time the best classification results obtained (an MCC of 0.24 for the holdout data) show that there is no obvious advantage to using the GBM over a simpler sampling strategy (see [SVM+undersampling](https://github.com/Meena-Mani/SECOM_class_imbalance/blob/master/secomdata_svm_undersampling.ipynb)).\n",
    "\n",
    "On the question of rebalancing with `sample_weight`, once again it is not clear that rebalancing gives better results. When the parameters were manually selected, both the default and weighted GBM showed results comparable to other classifiers such as the [Random Forest](https://github.com/Meena-Mani/SECOM_class_imbalance/blob/master/secomdata_rf.ipynb). The `hyperopt` selected the sample_weight over the default in at least 9/10 runs (not all runs are shown in Section IV) but the holdout MCC was not good (e.g. 0.004). For the gridsearch crossvalidation, results for default vs sample_weight were comparable.\n",
    "\n",
    "In previous exercises we had experimented with [SVM+oversampling using SMOTE](https://github.com/Meena-Mani/SECOM_class_imbalance/blob/master/secomdata_svm_smote.ipynb) and [SVM+undersampling](https://github.com/Meena-Mani/SECOM_class_imbalance/blob/master/secomdata_svm_undersampling.ipynb). Our experiments thus far indicate that SVM combined with a sampling strategy holds promise and we  will next look at a combination of oversampling (the minority class) and undersampling (the majority class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>VII. References and Further Reading </h3>\n",
    "\n",
    "<a name=\"ref1\"></a>[1] [Bergstra, James S., et al. “Algorithms for hyper-parameter optimization.” Advances in Neural Information Processing Systems. (2011)](http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FAAC58; margin-left: 0px; margin-right: 20px; padding-bottom: 8px; padding-left: 8px; padding-right: 8px; padding-top: 8px;\">\n",
    "\n",
    "\n",
    "Author:  Meena Mani  <br>\n",
    "email:   meenas.mailbag@gmail.com   <br> \n",
    "twitter: @meena_uvaca    <br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
